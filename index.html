<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Yimin Jiang</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

  </head>

  <body>

    <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Yimin Jiang</a>
        </div>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="index.html">Home</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </div>

    <div class="container">
    <div class="starter-template">
	<div class="row">
		<!-- <div class="col-md-3"> -->
			<!-- <img src="files/ymjiang.jpg" alt="Yimin Jiang" class="img-rounded" -->
            <!-- height="210px"/> -->
		<!-- </div> -->
		<div class="col-md-7">
			<h1>Yimin Jiang</h1>

      Email: jymthu (at) gmail (dot) com 
      
      [<a href="https://www.linkedin.com/in/yiminjiang/">LinkedIn</a>] </small>
        <!-- <p class="lead">Researcher<br> -->
			<!-- School of Computer Science and Technology<br> -->
			<!-- Tsinghua University -->
            <!-- </p> -->
    
    <br><br>

		</div>

		<div class="col-md-10">
		<p class="lead"><small>
        I am a researcher and a software engineer, working on large-scale AI infrastructure.
        Currently, I am interested in designing and optimizing the systems and algorithms for AGI, 
        with a focus on (Multi-modal) LLMs, Mixture-of-Experts (MoE), and Generative Models for Image/Video. 
        <br>

        <br> I work at ByteDance AML since 2021, where I lead the distributed training team 
        and also built the first engineering framework for LLM pretraining and RLHF-based alignment, known as the Seed Project.
        Prior to Seed, I was the tech lead of two core projects: (1) BytePS: collective communication optimizations for PyTorch and TensorFlow; 
        (2) MoE for NLP and multi-modality encoders: increasing the base model capacity with large number of experts.
        Both BytePS and MoE models have been deployed on thousands of training and serving GPUs for ByteDance businesses, such as Douyin/Search/RecSys, etc.

        <br><br> I have experience optimizing and diagnosing systems involving >10000 GPUs, and had deeply participated in training models reaching up to 10<sup>25</sup> FLOPs.

        <br><br>I received my PhD in Computer Science from Tsinghua University in 2021, and my BS from Nanjing University in 2016. <br>



        <hr>
        <!-- <h3>Research</h3>
        <p class="lead"><small>
        My research currently focuses on large language models.
        <br><br> -->



        <p class="lead">
          <b>Recent Papers</b><br> 
            <p class="lead"> <small>
              Teola: Towards End-to-End Optimization of LLM-based Applications.<br>
              <small> Xin Tan, <b>Yimin Jiang</b>, Yitao Yang, Hong Xu<br>
              Preprint [<a href="files/preprint_teola.pdf">PDF</a>]
              </small>
            </p>

            <p class="lead"> <small>
              Adaptive Gating in Mixture-of-Experts based Language Models.<br>
              <small> Jiamin Li, Qiang Su, Yitao Yang, <b>Yimin Jiang</b>, Cong Wang, Hong Xu<br>
              EMNLP 2023 [<a href="files/emnlp2023_adamoe.pdf">PDF</a>]
              </small>
            </p>

            <p class="lead"> <small>
              Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models.<br>
              <small> Juncai Liu, Jessie Hui Wang, <b>Yimin Jiang</b><br>
              SIGCOMM 2023 [<a href="files/sigcomm2023_janus.pdf">PDF</a>]
              </small>
            </p>

            <p class="lead"> <small>
              Accelerating Distributed MoE Training and Inference with Lina.<br>
              <small> Jiamin Li, <b>Yimin Jiang</b>, Yibo Zhu, Cong Wang, Hong Xu<br>
              ATC 2023 [<a href="files/atc23_lina.pdf">PDF</a>]
              </small>
            </p>

            <p class="lead"> <small>
              BytePS: A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters.<br>
              <small> <b>Yimin Jiang</b>, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, Chuanxiong Guo<br>
              OSDI 2020 [<a href="files/osdi2020_byteps.pdf">PDF</a>] [<a href="https://github.com/bytedance/byteps">CODE</a>]
              </small>
            </p>

        </p>

		</div>
	</div>

    </div>
    </div><!-- /.container -->

    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>
